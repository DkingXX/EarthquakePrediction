{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        # 每个网络层后面跟的都是数据的shape\n",
    "        self.rnn1 = nn.RNN(input_size=58, hidden_size=64, batch_first=True) # batchsize, length, H_in\n",
    "        self.dense = nn.Linear(61 * 64, 1)  # batchsize, length * H_out\n",
    "        self.sigmoid = nn.Sigmoid() # batchsize, 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 原始数据读进来是[data samples, H_in, length]，需要转换维度到[data samples, length, H_in]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.rnn1(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "        x = self.dense(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.rnn1 = nn.LSTM(input_size=58, hidden_size=128, dropout=0.2, batch_first=True) # batchsize, length, H_in\n",
    "        self.rnn2 = nn.LSTM(input_size=128, hidden_size=128, dropout=0.2, batch_first=True)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1) # batchsize, length\n",
    "        self.dense = nn.Linear(61, 1)\n",
    "        self.sigmoid = nn.Sigmoid() # batchsize, 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.rnn1(x)\n",
    "        x, _ = self.rnn2(x)\n",
    "        x = self.pooling(x)\n",
    "        x = x[:, :, 0]\n",
    "        x = self.dense(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "\n",
    "        self.rnn1 = nn.LSTM(input_size=58, hidden_size=128, dropout=0.2, batch_first=True, bidirectional=True)# batchsize, length, H_in\n",
    "        self.rnn2 = nn.LSTM(input_size=128 * 2, hidden_size=128, dropout=0.2, batch_first=True, bidirectional=True)# batchsize, length, 2 * H_out\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1) # batchsize, length\n",
    "        self.dense = nn.Linear(61, 1) # batchsize, 1\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.rnn1(x)\n",
    "        x, _ = self.rnn2(x)\n",
    "        x = self.pooling(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "        x = self.dense(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X, Y = [], []\n",
    "    data = np.load('./normal_data_processed.pkl', allow_pickle=True)\n",
    "    print(data.shape)\n",
    "    for item in data: # 遍历normal_data_processed里面的数据，整理成numpy array格式的数据，方便处理\n",
    "        if item.shape != (58, 61) or np.sum(np.isnan(item)):\n",
    "            continue\n",
    "        X.append(np.array(item, dtype=np.float32))\n",
    "\n",
    "    Y.extend(np.zeros(len(X))) # 正常数据对应的label为0\n",
    "\n",
    "    data1 = np.load('./earthquakes_data_processed.pkl', allow_pickle=True)\n",
    "\n",
    "    for item in data1: # 遍历地震的数据\n",
    "        if item.shape != (58, 61) or np.sum(np.isnan(item)):\n",
    "            continue\n",
    "        X.append(np.array(item, dtype=np.float32))\n",
    "    Y.extend(np.ones(len(X) - len(Y))) # 正常数据对应的label为1\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    Y = np.array(Y, dtype=np.float32)\n",
    "    # 因为X是一整块正常的数据和一整块地震数据，这里做shuffle，把X和Y以相同的顺序打乱\n",
    "    shuffle_ix = np.random.permutation(np.arange(len(X)))\n",
    "    data = X[shuffle_ix]\n",
    "    label = Y[shuffle_ix]\n",
    "    return data, label\n",
    "\n",
    "# 生成dataloader，dataloader是torch框架中数据管道的类，用于快速遍历数据\n",
    "def get_dataloader(test=False):\n",
    "    # 固定随机种子，保证每次数据分割是一致的\n",
    "    torch.manual_seed(123)\n",
    "    torch.random.manual_seed(123)\n",
    "\n",
    "    x, y = load_data()\n",
    "    dataset = TensorDataset(torch.tensor(x), torch.tensor(y))# 得到数据集\n",
    "\n",
    "    train_dataset, valid_dataset, test_dataset = random_split(dataset, [int(0.7 * len(x)), int(0.2 * len(x)),\n",
    "                                                                        len(x) - int(0.7 * len(x)) - int(0.2 * len(x))])\n",
    "    if test: # 测试的话，意味着有训练好的模型，只需要在测试数据上测试一下即可\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=1, num_workers=4, shuffle=False) # 制作\n",
    "        return test_dataloader\n",
    "    else: # 训练的话，需要训练计和验证集\n",
    "        # 转换成dataloader\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=128, num_workers=4, shuffle=True)\n",
    "        valid_dataloader = DataLoader(valid_dataset, batch_size=128, num_workers=4, shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=128, num_workers=4, shuffle=False)\n",
    "        return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 1e-3\n",
    "model_dict = {\n",
    "    'VanillaRNN': VanillaRNN(),\n",
    "    'LSTM': LSTMModel(),\n",
    "    'BiLSTM': BiLSTMModel(),\n",
    "}\n",
    "model_name = 'LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 训练过程\n",
    "def training():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 确认执行设备\n",
    "    model = model_dict[model_name]\n",
    "    train_dataloader, valid_dataloader, test_dataloader = get_dataloader(False) # 得到数据的迭代器，这里训练用train_dataloader\n",
    "    # 验证用valid_dataloader 测试用test_dataloader\n",
    "    loss_func = torch.nn.BCELoss().to(device) # loss计算函数\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) # 模型参数优化器\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99) # 学习率调整器\n",
    "\n",
    "    best_loss = 100000 # 用于判断验证集loss最小的参数\n",
    "    history = {'train loss': [], 'test loss': [], 'train acc': [], 'test acc': []} # 保存历史loss\n",
    "    if not os.path.exists('./checkpoints'): # 创建模型保存文件的文件夹\n",
    "        os.mkdir('./checkpoints')\n",
    "    for epoch in range(epochs):\n",
    "        mean_train_loss = [] # 保存这个epoch中的训练集loss，用于绘图\n",
    "        mean_test_loss = []# 保存历史的验证集loss，用于绘图\n",
    "\n",
    "        labels = []\n",
    "        preds = []\n",
    "        model.train() # model切换为train模式\n",
    "        for item in tqdm(train_dataloader): # 遍历数据集，获取Sample\n",
    "            optimizer.zero_grad() # 清除优化器的梯度，这是每次更新参数必需的操作\n",
    "            value, label = item[0], item[1] # 获取数据集sample中的value和label\n",
    "            output = model(value.to(device)) # 得到输出\n",
    "\n",
    "            loss = loss_func(output.squeeze(), label.to(device)) # 由 output和label计算loss\n",
    "            loss.backward() # 由loss进行BP得到梯度\n",
    "            optimizer.step() # 优化器更新参数\n",
    "            mean_train_loss.append(loss.detach().cpu().numpy()) # 把loss放入历史信息\n",
    "            preds.extend(np.argmax(output.detach().cpu().numpy(), 1)) # 保存预测值\n",
    "            labels.extend(label.numpy()) # 保存label，以便计算当前轮的准确率\n",
    "\n",
    "        test_preds = []\n",
    "        test_labels = []\n",
    "        model.eval() # model切换为评估模式\n",
    "        for item in valid_dataloader: # 遍历验证集\n",
    "            value, label = item[0], item[1] # 获取数据\n",
    "            output = model(value.to(device)) # 得到输出\n",
    "            loss = loss_func(output.squeeze(), label.to(device)) # 计算loss\n",
    "            mean_test_loss.append(loss.detach().cpu().numpy()) # 保存loss\n",
    "            test_preds.extend(np.argmax(output.detach().cpu().numpy(), 1))\n",
    "            test_labels.extend(label.numpy())\n",
    "        # 上面的mean_train_loss和mean_test_loss是保存一个epoch内的loss信息，历史信息保存的是每个epoch的loss\n",
    "        history['train loss'].append(np.mean(mean_train_loss))\n",
    "        history['test loss'].append(np.mean(mean_test_loss))\n",
    "        history['train acc'].append(accuracy_score(preds, labels))\n",
    "        history['test acc'].append(accuracy_score(test_preds, test_labels))\n",
    "        print(  # 打印该epoch的两个loss和两个accuracy\n",
    "            'Epoch {}/{}: train loss is {:.4f}, test loss is {:.4f}, train acc is {:.4f}, test acc is {:.4f}'\n",
    "                .format(epoch + 1, 100, np.mean(mean_train_loss), np.mean(mean_test_loss),\n",
    "                        accuracy_score(preds, labels), accuracy_score(test_preds, test_labels)))\n",
    "        print(np.mean(labels), np.mean(test_labels))\n",
    "        # lr_scheduler.step()  # 更新学习率\n",
    "        if np.mean(mean_test_loss) < best_loss: # 这是判断保存验证集loss最小模型的操作\n",
    "            best_loss = np.mean(mean_test_loss)\n",
    "            torch.save(model, os.path.join('./checkpoints/', model_name))\n",
    "    # 画出历史两个loss的折线图并保存下来\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epochs), history['train loss']) # 折线图绘制\n",
    "    plt.plot(np.arange(0, epochs), history['test loss'])\n",
    "    plt.legend(['train loss', 'test loss'])\n",
    "    plt.title('Loss') # 标题\n",
    "    plt.savefig('loss_{}.png'.format(model_name)) # 保存\n",
    "    plt.show() # 展示图片\n",
    "\n",
    "    # 画出历史两个acc的折线图并保存下来\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epochs), history['train acc']) # 折线图绘制\n",
    "    plt.plot(np.arange(0, epochs), history['test acc'])\n",
    "    plt.legend(['train acc', 'test acc'])\n",
    "    plt.title('Accuracy') # 标题\n",
    "    plt.savefig('Accuracy_{}.png'.format(model_name)) # 保存\n",
    "    plt.show() # 展示图片\n",
    "\n",
    "    ############ testing ############\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    model.eval()  # model切换为评估模式\n",
    "    for item in test_dataloader:  # 遍历验证集\n",
    "        value, label = item[0], item[1]  # 获取数据\n",
    "        output = model(value.to(device))  # 得到输出\n",
    "        test_preds.extend(np.argmax(output.detach().cpu().numpy(), 1))\n",
    "        test_labels.extend(label.numpy())\n",
    "\n",
    "    print(classification_report(test_labels, test_preds, digits=4)) # 准确率，召回率，f1 score\n",
    "\n",
    "def test():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 确认执行设备\n",
    "    model = torch.load(os.path.join('./checkpoints/', model_name))\n",
    "    test_dataloader = get_dataloader(True) # 仅获得测试数据的dataloader\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    model.eval()  # model切换为评估模式\n",
    "    for item in test_dataloader:  # 遍历验证集\n",
    "        value, label = item[0], item[1]  # 获取数据\n",
    "        output = model(value.to(device))  # 得到输出\n",
    "        test_preds.extend(np.argmax(output.detach().cpu().numpy(), 1))\n",
    "        test_labels.extend(label.numpy())\n",
    "\n",
    "    print(classification_report(test_labels, test_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
